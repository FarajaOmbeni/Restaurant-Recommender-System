# -*- coding: utf-8 -*-
"""recommend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XgteubEEBC42jCACI-AIBP0QSu8czha2
"""

from google.colab import drive
drive.mount('/content/drive/')

!pip install -q keras
import keras

# import libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import csv

df_1 = pd.read_csv('/content/drive/MyDrive/datasets/geoplaces2.csv')
df_2 = pd.read_csv('/content/drive/MyDrive/datasets/rating_final.csv')
df_1

df_2

#merge the 2 dataframes
ratings = pd.merge(df_1, df_2)
ratings = ratings[['placeID', 'name', 'userID', 'rating']]
#take needed colmns
ratings['userID'] = ratings['userID'].str[1:]
#remove the 'U' from the userID's
ratings.dropna(inplace=True)
ratings.head()

#Explarotary Data analysis
ratings.info()

#we have 1161 ratings and 4 columns. Let's check how many restaurants we have
ratings.name.nunique()

#Data visualization
sns.countplot(x=ratings.rating)

#The following heatmap shows the correlation between the number of ratings and average rating for each place,
#with warmer colors indicating a stronger correlation
# Group the data by item ID and calculate the number of ratings and average rating for each item
item_ratings = ratings.groupby('placeID').agg({'rating': ['count', 'mean']}).reset_index()
# Rename the columns
item_ratings.columns = ['placeID', 'num_ratings', 'avg_rating']
# Create a pivot table showing the correlation between the number of ratings and average rating for each book
pivot = pd.pivot_table(item_ratings, index='num_ratings', columns='avg_rating', values='placeID')
# Create a heatmap of the pivot table
plt.figure(figsize=(20,20))
sns.heatmap(pivot, cmap='RdBu_r')

#popularity based recommender system
#function to calculate popularity stats
def popularity_based_rec(df, group_col, rating_col):
  #group by title and get sixe, num and mean values
  grouped = df.groupby(group_col).agg({rating_col:[np.size, np.sum, np.mean]})

  #most popular mean value on top
  popular = grouped.sort_values((rating_col, 'mean'), ascending=False)
  total_sum = grouped[rating_col]['sum'].sum()

  #percentage weight
  popular['percentage'] = popular[rating_col]['sum'].div(total_sum) * 100
  return popular.sort_values(('percentage'), ascending=False)

#show 5 popular restaurants
popularity_stats = popularity_based_rec(ratings, 'name', 'rating')
popularity_stats.head()

#nearest neighbor recommender system (collaborative filtering)
# sort the restaurants from largest to smallest according to its mean ratings
itemProperties = ratings.groupby("placeID").agg({"rating": [np.size, np.mean]})
itemProperties.head()

# calculate their percentages
itemNumRatings = pd.DataFrame(itemProperties["rating"]["size"])
itemNormalizedNumRatings = itemNumRatings.apply(lambda x: (x-np.min(x)) / (np.max(x) - np.min(x)))
itemNormalizedNumRatings.tail() # show last 5 entries

ratings.to_csv('/content/drive/MyDrive/datasets/ratings.csv')

df = pd.read_csv('/content/drive/MyDrive/datasets/ratings.csv')
df.head()

# store all restaurants in a dictionary with their id's, names ratings, number of ratings and average ratings
itemDict = {} # create an empty item Dictionary
# Read in the ratings data from the CSV file
with open('/content/drive/MyDrive/datasets/ratings.csv', mode='r') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    next(csv_reader)  # skip the first row
    for row in csv_reader:
        if row[1] == '' or row[2] == '' or row[3] == '' or row[4] == '':
          continue
        # get the right columns
        itemID = int(row[1])
        name = row[2]
        userID = int(row[3])
        rating = int(row[4])

        if itemID not in itemDict:
            itemDict[itemID] = {'name': name, 'ratings': [], 'numRatings': 0, 'totalRating': 0}
        itemDict[itemID]['ratings'].append(rating)
        itemDict[itemID]['numRatings'] += 1
        itemDict[itemID]['totalRating'] += rating

# Calculate the average rating for each item
for itemID in itemDict:
    item = itemDict[itemID]
    name = item['name']
    ratings = item['ratings']
    numRatings = item['numRatings']
    totalRating = item['totalRating']
    avgRating = totalRating / numRatings
    itemDict[itemID] = {'name': name, 'ratings': ratings, 'numRatings': numRatings, 'avgRating': avgRating}

itemDict

# function that finds the distance of an item from another item
def ComputeDistance(a, b):
    # Find the common ratings for both item
    common_ratings = [rating for rating in a['ratings'] if rating in b['ratings']]

    # If there are no common ratings, the distance is infinity
    if len(common_ratings) == 0:
        return float('inf')
    # If the lists of ratings are not the same length, return infinity
    if len(a['ratings']) != len(b['ratings']):
        return float('inf')
    # Calculate the sum of the squared differences between the ratings
    sum_squared_differences = sum([(a['ratings'][i] - b['ratings'][i]) ** 2 for i in range(len(common_ratings))])

    # Return the square root of the sum of squared differences, which is the distance between the two items
    return sum_squared_differences ** 0.5

# function to get K-Nearest Neighbors
def getNeighbors(itemID, K):
    # Get the item object for the given item ID
    target_item = itemDict[itemID]

    # Create a list of tuples (distance, itemID) for all items in the dictionary
    distances = [(ComputeDistance(target_item, itemDict[itemID]), itemID) for itemID in itemDict if itemDict[itemID]['name'] != target_item['name']]
    # Sort the list of tuples by distance (ascending)
    distances.sort()

    # Return the K nearest neighbors (item with the lowest distances)
    return distances[:K]

# get the smallest distances as a list
neighbors = getNeighbors(134999, 30)
# Print the item names and distances of the nearest neighbors
for distance, itemID in neighbors:
    print(f"{itemDict[itemID]['name']}: {distance:.2f}")

#USER BASED RECOMMENDER SYSTEM
ratings = pd.read_csv('/content/drive/MyDrive/datasets/ratings.csv')

#Matrix Factorization (pivot table)
itemratings = ratings.pivot_table(index=['userID'], columns=['name'], values='rating')
itemratings.head()

# an example of the correaltion between 'Cafe Chaires' and 'la parroquia'
item=itemratings['Cafe Chaires']
itemratings[["Cafe Chaires","la parroquia"]].corr()

# show items with most correlation
itemratings.corrwith(item).sort_values(ascending=False).to_frame('corr').dropna()

# show item with least correlation
itemratings.corrwith(item).sort_values(ascending=True).to_frame('corr').dropna()

# show a sample of random 15 items and see their correlation
itemratings.corrwith(item).to_frame('corr').dropna().sample(15).dropna()